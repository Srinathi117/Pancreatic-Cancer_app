{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW3Rn9RN0LMwhu601O/lUC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srinathi117/Pancreatic-Cancer_app/blob/main/Pancreatic_prediction_GNN_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile, os\n",
        "\n",
        "print(\"‚¨ÜÔ∏è Upload TWO ZIP FILES (1. Image Dataset ZIP, 2. CSV ZIP)\")\n",
        "uploaded = files.upload()  # Select BOTH zip files at SAME time\n",
        "\n",
        "# Create target folders\n",
        "DATASET_DIR = \"/content/dataset\"\n",
        "CSV_DIR = \"/content/pc_excel\"\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "os.makedirs(CSV_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\nProcessing uploaded zip files...\\n\")\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    filepath = f\"/content/{filename}\"\n",
        "\n",
        "    # Extract based on file type name\n",
        "    if \"csv\" in filename.lower() or \"excel\" in filename.lower():\n",
        "        extract_path = CSV_DIR\n",
        "    else:\n",
        "        extract_path = DATASET_DIR\n",
        "\n",
        "    # Unzip\n",
        "    with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    print(f\"üìÇ '{filename}' extracted to: {extract_path}\")\n",
        "\n",
        "print(\"\\n‚úîÔ∏è Files extracted successfully!\")\n",
        "print(\"Dataset folder contains:\", os.listdir(DATASET_DIR))\n",
        "print(\"CSV/Excel folder contains:\", os.listdir(CSV_DIR))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "nEq2JwlPuSSV",
        "outputId": "18762bfe-4f68-4e8b-df63-9b68015dc001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨ÜÔ∏è Upload TWO ZIP FILES (1. Image Dataset ZIP, 2. CSV ZIP)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-39ce4cb9-c903-4eec-b2d2-0744c2c9ccaa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-39ce4cb9-c903-4eec-b2d2-0744c2c9ccaa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving archive (5).zip to archive (5) (1).zip\n",
            "Saving pc excel.zip to pc excel.zip\n",
            "\n",
            "Processing uploaded zip files...\n",
            "\n",
            "üìÇ 'archive (5) (1).zip' extracted to: /content/dataset\n",
            "üìÇ 'pc excel.zip' extracted to: /content/pc_excel\n",
            "\n",
            "‚úîÔ∏è Files extracted successfully!\n",
            "Dataset folder contains: ['DATASET']\n",
            "CSV/Excel folder contains: ['pancreatic_cancer_prediction_sample.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# PANCREATIC CANCER GNN: CANCER vs NORMAL\n",
        "# ==============================================\n",
        "\n",
        "# ---------- 0. INSTALL LIBRARIES (RUN ONCE) ----------\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q pandas\n",
        "\n",
        "# ---------- 1. IMPORTS & BASIC SETUP ----------\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ---------- 2. PATHS ----------\n",
        "# Your folder structure:\n",
        "# /content/dataset/DATASET/train/Cancer, Normal\n",
        "# /content/dataset/DATASET/test/Cancer, Normal\n",
        "\n",
        "data_root = \"/content/dataset/DATASET\"   # main dataset folder\n",
        "csv_path = \"/content/pc_excel/pancreatic_cancer_prediction_sample.csv\"  # second dataset (tabular)\n",
        "\n",
        "print(\"Inside DATASET:\", os.listdir(data_root))\n",
        "\n",
        "train_root_base = os.path.join(data_root, \"train\")\n",
        "test_root_base  = os.path.join(data_root, \"test\")\n",
        "\n",
        "print(\"Raw train_root_base:\", train_root_base)\n",
        "print(\"Raw test_root_base:\", test_root_base)\n",
        "\n",
        "def find_class_root(root):\n",
        "    \"\"\"If there is only one subfolder (like 'train' or 'test'), go one level deeper.\"\"\"\n",
        "    subs = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n",
        "    print(f\"\\nChecking root: {root}\")\n",
        "    print(\" Subfolders:\", subs)\n",
        "    if len(subs) == 1:\n",
        "        new_root = os.path.join(root, subs[0])\n",
        "        print(\" -> Only one subfolder, using inner folder as class root:\", new_root)\n",
        "        return new_root\n",
        "    else:\n",
        "        print(\" -> Using this as class root\")\n",
        "        return root\n",
        "\n",
        "train_root = find_class_root(train_root_base)\n",
        "test_root  = find_class_root(test_root_base)\n",
        "\n",
        "print(\"\\nFinal train_root:\", train_root)\n",
        "print(\"Final test_root:\", test_root)\n",
        "\n",
        "print(\"\\nTrain classes folders:\", os.listdir(train_root))\n",
        "print(\"Test classes folders:\", os.listdir(test_root))\n",
        "\n",
        "# ---------- 2.1 LOAD CSV (SECOND DATASET) ----------\n",
        "if os.path.exists(csv_path):\n",
        "    csv_df = pd.read_csv(csv_path)\n",
        "    print(\"\\nLoaded CSV:\", csv_path)\n",
        "    print(\"CSV shape:\", csv_df.shape)\n",
        "    print(csv_df.head())\n",
        "else:\n",
        "    csv_df = None\n",
        "    print(\"\\nNo CSV found at:\", csv_path)\n",
        "\n",
        "# ---------- 3. IMAGE TRANSFORMS ----------\n",
        "img_size = 224\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])\n",
        "\n",
        "# ---------- 4. DATASETS ----------\n",
        "train_dataset = datasets.ImageFolder(root=train_root, transform=transform)\n",
        "test_dataset  = datasets.ImageFolder(root=test_root,  transform=transform)\n",
        "\n",
        "print(\"\\nTrain classes:\", train_dataset.classes)\n",
        "print(\"Test classes:\", test_dataset.classes)\n",
        "\n",
        "# Ensure same class mapping\n",
        "assert train_dataset.classes == test_dataset.classes, \"Train/Test classes mismatch!\"\n",
        "\n",
        "class_names = train_dataset.classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(\"\\nClass index mapping:\")\n",
        "for idx, name in enumerate(class_names):\n",
        "    print(idx, \"->\", name)\n",
        "print(\"Number of classes:\", num_classes)\n",
        "\n",
        "# Identify which class index is CANCER automatically\n",
        "CANCER_CLASS_INDICES = [i for i, c in enumerate(class_names) if \"cancer\" in c.lower() or \"tumor\" in c.lower()]\n",
        "if not CANCER_CLASS_INDICES:\n",
        "    print(\"\\n‚ö†Ô∏è Warning: No class name contains 'Cancer' or 'Tumor'.\")\n",
        "else:\n",
        "    print(\"\\nCancer class indices:\", CANCER_CLASS_INDICES)\n",
        "\n",
        "# Merge train + test into one dataset for building a single graph\n",
        "full_dataset = ConcatDataset([train_dataset, test_dataset])\n",
        "print(\"\\nTotal images (train + test):\", len(full_dataset))\n",
        "\n",
        "# ---------- 5. FEATURE EXTRACTION (RESNET18 BACKBONE) ----------\n",
        "loader = DataLoader(full_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "backbone.fc = nn.Identity()   # remove final classifier -> feature extractor\n",
        "backbone = backbone.to(device)\n",
        "backbone.eval()\n",
        "\n",
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        feats = backbone(imgs)    # [B, feat_dim]\n",
        "        all_features.append(feats.cpu())\n",
        "        all_labels.append(labels)\n",
        "\n",
        "X = torch.cat(all_features, dim=0)   # [N, feat_dim]\n",
        "y = torch.cat(all_labels, dim=0)     # [N]\n",
        "\n",
        "print(\"\\nFeature matrix shape:\", X.shape)\n",
        "print(\"Labels shape:\", y.shape)\n",
        "\n",
        "# ---------- 6. BUILD k-NN GRAPH ----------\n",
        "k = 8   # neighbors per node\n",
        "features_np = X.numpy()\n",
        "\n",
        "nbrs = NearestNeighbors(n_neighbors=k + 1, metric='euclidean').fit(features_np)\n",
        "distances, indices = nbrs.kneighbors(features_np)\n",
        "\n",
        "edge_index_list = []\n",
        "N = indices.shape[0]\n",
        "\n",
        "for i in range(N):\n",
        "    for j in indices[i, 1:]:   # skip itself\n",
        "        edge_index_list.append([i, j])\n",
        "        edge_index_list.append([j, i])   # undirected\n",
        "\n",
        "edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
        "print(\"edge_index shape:\", edge_index.shape)  # [2, num_edges]\n",
        "\n",
        "# ---------- 7. TRAIN/VAL/TEST SPLIT ----------\n",
        "idx = np.arange(N)\n",
        "\n",
        "# 20% test\n",
        "idx_train_val, idx_test = train_test_split(\n",
        "    idx, test_size=0.2, stratify=y.numpy(), random_state=42\n",
        ")\n",
        "\n",
        "# From remaining: 20% as val => 64% train, 16% val, 20% test\n",
        "idx_train, idx_val = train_test_split(\n",
        "    idx_train_val, test_size=0.2, stratify=y.numpy()[idx_train_val], random_state=42\n",
        ")\n",
        "\n",
        "train_mask = torch.zeros(N, dtype=torch.bool)\n",
        "val_mask   = torch.zeros(N, dtype=torch.bool)\n",
        "test_mask  = torch.zeros(N, dtype=torch.bool)\n",
        "\n",
        "train_mask[idx_train] = True\n",
        "val_mask[idx_val]     = True\n",
        "test_mask[idx_test]   = True\n",
        "\n",
        "print(f\"\\nTrain nodes: {train_mask.sum().item()}, \"\n",
        "      f\"Val nodes: {val_mask.sum().item()}, \"\n",
        "      f\"Test nodes: {test_mask.sum().item()}\")\n",
        "\n",
        "# ---------- 8. GRAPH DATA OBJECT ----------\n",
        "graph_data = Data(\n",
        "    x=X,\n",
        "    edge_index=edge_index,\n",
        "    y=y,\n",
        "    train_mask=train_mask,\n",
        "    val_mask=val_mask,\n",
        "    test_mask=test_mask\n",
        ").to(device)\n",
        "\n",
        "# ---------- 9. DEFINE GNN MODEL ----------\n",
        "class PancreaticGCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "in_channels = graph_data.x.size(1)\n",
        "hidden_channels = 128\n",
        "\n",
        "model = PancreaticGCN(\n",
        "    in_channels=in_channels,\n",
        "    hidden_channels=hidden_channels,\n",
        "    num_classes=num_classes,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\nGNN model:\\n\", model)\n",
        "\n",
        "# ---------- 10. TRAIN & EVALUATE ----------\n",
        "def train_step():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(graph_data.x, graph_data.edge_index)\n",
        "    loss = criterion(out[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(mask):\n",
        "    model.eval()\n",
        "    out = model(graph_data.x, graph_data.edge_index)\n",
        "    pred = out.argmax(dim=1)\n",
        "    correct = (pred[mask] == graph_data.y[mask]).sum().item()\n",
        "    total = int(mask.sum())\n",
        "    return correct / total if total > 0 else 0.0\n",
        "\n",
        "epochs = 50   # increase if needed\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train_step()\n",
        "    train_acc = accuracy(graph_data.train_mask)\n",
        "    val_acc   = accuracy(graph_data.val_mask)\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | \"\n",
        "              f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "test_acc = accuracy(graph_data.test_mask)\n",
        "print(f\"\\n‚úÖ Final Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Label distribution on test\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(graph_data.x, graph_data.edge_index)\n",
        "    pred = out.argmax(dim=1)\n",
        "\n",
        "true_labels = graph_data.y[graph_data.test_mask].cpu().numpy()\n",
        "pred_labels = pred[graph_data.test_mask].cpu().numpy()\n",
        "\n",
        "print(\"\\nTrue label counts (test):\", Counter(true_labels))\n",
        "print(\"Pred label counts (test):\", Counter(pred_labels))\n",
        "\n",
        "# ---------- 11. SINGLE IMAGE PREDICTION (CANCER vs NORMAL) ----------\n",
        "from google.colab import files as colab_files\n",
        "\n",
        "def predict_pancreatic_image(img_path):\n",
        "    \"\"\"\n",
        "    Use the trained backbone + GNN model to predict class for one image.\n",
        "    (Approximate with a self-loop graph for this single node.)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    backbone.eval()\n",
        "\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img_t = transform(img).unsqueeze(0).to(device)   # [1, 3, H, W]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # CNN feature\n",
        "        feat = backbone(img_t)                      # [1, feat_dim]\n",
        "\n",
        "        # Self-loop graph for single node\n",
        "        edge_index_new = torch.tensor([[0], [0]], dtype=torch.long).to(device)\n",
        "\n",
        "        out = model(feat, edge_index_new)           # [1, num_classes]\n",
        "        probs = torch.softmax(out, dim=1)[0]        # [num_classes]\n",
        "        pred_idx = int(torch.argmax(probs))\n",
        "        pred_class = class_names[pred_idx]\n",
        "        conf = float(probs[pred_idx])\n",
        "\n",
        "    return pred_idx, pred_class, conf\n",
        "\n",
        "def cancer_or_not(pred_idx):\n",
        "    if pred_idx in CANCER_CLASS_INDICES:\n",
        "        return \"‚ö†Ô∏è The model predicts: CANCER.\"\n",
        "    else:\n",
        "        return \"‚úÖ The model predicts: NO CANCER (normal).\"\n",
        "\n",
        "print(\"\\nReady for single-image prediction. Upload CT/MRI image.\")\n",
        "\n",
        "uploaded = colab_files.upload()   # choose image file(s)\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    img_path = \"/content/\" + filename\n",
        "\n",
        "    pred_idx, pred_class, conf = predict_pancreatic_image(img_path)\n",
        "    result_text = cancer_or_not(pred_idx)\n",
        "\n",
        "    print(\"\\n===================================\")\n",
        "    print(f\"Image: {filename}\")\n",
        "    print(f\"Predicted class label: {pred_class} (index {pred_idx})\")\n",
        "    print(f\"Confidence: {conf:.4f}\")\n",
        "    print(result_text)\n",
        "    print(\"===================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bCAe0IosyVtg",
        "outputId": "45159049-e57c-44e8-e4dd-16c4e8b1467f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing device: cpu\n",
            "Inside DATASET: ['test', 'train']\n",
            "Raw train_root_base: /content/dataset/DATASET/train\n",
            "Raw test_root_base: /content/dataset/DATASET/test\n",
            "\n",
            "Checking root: /content/dataset/DATASET/train\n",
            " Subfolders: ['train']\n",
            " -> Only one subfolder, using inner folder as class root: /content/dataset/DATASET/train/train\n",
            "\n",
            "Checking root: /content/dataset/DATASET/test\n",
            " Subfolders: ['test']\n",
            " -> Only one subfolder, using inner folder as class root: /content/dataset/DATASET/test/test\n",
            "\n",
            "Final train_root: /content/dataset/DATASET/train/train\n",
            "Final test_root: /content/dataset/DATASET/test/test\n",
            "\n",
            "Train classes folders: ['normal', 'pancreatic_tumor']\n",
            "Test classes folders: ['normal', 'pancreatic_tumor']\n",
            "\n",
            "Loaded CSV: /content/pc_excel/pancreatic_cancer_prediction_sample.csv\n",
            "CSV shape: (50000, 24)\n",
            "         Country  Age  Gender  Smoking_History  Obesity  Diabetes  \\\n",
            "0         Canada   64  Female                0        0         0   \n",
            "1   South Africa   77    Male                1        1         0   \n",
            "2          India   71  Female                0        0         0   \n",
            "3        Germany   56    Male                0        0         0   \n",
            "4  United States   82  Female                0        0         0   \n",
            "\n",
            "   Chronic_Pancreatitis  Family_History  Hereditary_Condition  Jaundice  ...  \\\n",
            "0                     0               0                     0         0  ...   \n",
            "1                     0               0                     0         0  ...   \n",
            "2                     0               0                     0         0  ...   \n",
            "3                     0               1                     0         1  ...   \n",
            "4                     0               1                     0         0  ...   \n",
            "\n",
            "   Stage_at_Diagnosis  Survival_Time_Months  Treatment_Type  Survival_Status  \\\n",
            "0           Stage III                    13         Surgery                0   \n",
            "1           Stage III                    13    Chemotherapy                0   \n",
            "2            Stage IV                     3    Chemotherapy                1   \n",
            "3            Stage IV                     6       Radiation                0   \n",
            "4            Stage IV                     9    Chemotherapy                1   \n",
            "\n",
            "  Alcohol_Consumption  Physical_Activity_Level Diet_Processed_Food  \\\n",
            "0                   0                   Medium                 Low   \n",
            "1                   1                   Medium              Medium   \n",
            "2                   0                   Medium                High   \n",
            "3                   1                      Low                 Low   \n",
            "4                   0                      Low              Medium   \n",
            "\n",
            "   Access_to_Healthcare  Urban_vs_Rural Economic_Status  \n",
            "0                  High           Urban             Low  \n",
            "1                Medium           Urban             Low  \n",
            "2                   Low           Rural          Middle  \n",
            "3                Medium           Rural          Middle  \n",
            "4                Medium           Rural             Low  \n",
            "\n",
            "[5 rows x 24 columns]\n",
            "\n",
            "Train classes: ['normal', 'pancreatic_tumor']\n",
            "Test classes: ['normal', 'pancreatic_tumor']\n",
            "\n",
            "Class index mapping:\n",
            "0 -> normal\n",
            "1 -> pancreatic_tumor\n",
            "Number of classes: 2\n",
            "\n",
            "Cancer class indices: [1]\n",
            "\n",
            "Total images (train + test): 1411\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 70.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature matrix shape: torch.Size([1411, 512])\n",
            "Labels shape: torch.Size([1411])\n",
            "edge_index shape: torch.Size([2, 22576])\n",
            "\n",
            "Train nodes: 902, Val nodes: 226, Test nodes: 283\n",
            "\n",
            "GNN model:\n",
            " PancreaticGCN(\n",
            "  (conv1): GCNConv(512, 128)\n",
            "  (conv2): GCNConv(128, 2)\n",
            ")\n",
            "Epoch 001 | Loss: 1.0112 | Train Acc: 0.5211 | Val Acc: 0.5221\n",
            "Epoch 005 | Loss: 0.2552 | Train Acc: 0.9534 | Val Acc: 0.9071\n",
            "Epoch 010 | Loss: 0.1082 | Train Acc: 0.9889 | Val Acc: 0.9779\n",
            "Epoch 015 | Loss: 0.0757 | Train Acc: 0.9889 | Val Acc: 0.9779\n",
            "Epoch 020 | Loss: 0.0595 | Train Acc: 0.9889 | Val Acc: 0.9779\n",
            "Epoch 025 | Loss: 0.0411 | Train Acc: 0.9889 | Val Acc: 0.9779\n",
            "Epoch 030 | Loss: 0.0404 | Train Acc: 0.9889 | Val Acc: 0.9779\n",
            "Epoch 035 | Loss: 0.0377 | Train Acc: 0.9878 | Val Acc: 0.9912\n",
            "Epoch 040 | Loss: 0.0273 | Train Acc: 0.9889 | Val Acc: 0.9779\n",
            "Epoch 045 | Loss: 0.0304 | Train Acc: 0.9878 | Val Acc: 0.9912\n",
            "Epoch 050 | Loss: 0.0268 | Train Acc: 0.9878 | Val Acc: 0.9912\n",
            "\n",
            "‚úÖ Final Test Accuracy: 0.9823\n",
            "\n",
            "True label counts (test): Counter({np.int64(1): 153, np.int64(0): 130})\n",
            "Pred label counts (test): Counter({np.int64(1): 154, np.int64(0): 129})\n",
            "\n",
            "Ready for single-image prediction. Upload CT/MRI image.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ecc6766f-c22c-4242-a962-93fcd9df1e6f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ecc6766f-c22c-4242-a962-93fcd9df1e6f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1-005.jpg to 1-005.jpg\n",
            "\n",
            "===================================\n",
            "Image: 1-005.jpg\n",
            "Predicted class label: normal (index 0)\n",
            "Confidence: 0.9980\n",
            "‚úÖ The model predicts: NO CANCER (normal).\n",
            "===================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== SAVE TRAINED MODEL (CNN + GNN) =====\n",
        "import torch\n",
        "\n",
        "save_path = \"/content/pancreas_gnn.pth\"\n",
        "\n",
        "torch.save({\n",
        "    \"backbone_state_dict\": backbone.state_dict(),\n",
        "    \"gcn_state_dict\": model.state_dict(),\n",
        "    \"class_names\": class_names,\n",
        "}, save_path)\n",
        "\n",
        "print(\"Model saved to:\", save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edhXurz85djf",
        "outputId": "54d7bb03-2510-40c3-8ab1-0e0d0f4e3ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/pancreas_gnn.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/pancreas_gnn.pth\")\n"
      ],
      "metadata": {
        "id": "S4Hiwr4U5iY1",
        "outputId": "6a17e1d3-0d57-48ac-c86d-a0a092beef24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_073223f5-a3b9-4239-aa51-d8ef901e9867\", \"pancreas_gnn.pth\", 45048913)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}